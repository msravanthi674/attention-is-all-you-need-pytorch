# ğŸš€ Transformer from Scratch (PyTorch)

> ğŸ“– Implementation of the paper *Attention Is All You Need* (Vaswani et al., 2017)  
> [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)

This project is a **from-scratch PyTorch implementation** of the **Transformer architecture**, faithfully following the original paper.  
It includes **Encoder + Decoder stacks**, **Multi-Head Attention**, **Positional Encodings**, and a **training pipeline** for experimentation.


![Python](https://img.shields.io/badge/Python-3.10-blue?logo=python)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red?logo=pytorch)
![NumPy](https://img.shields.io/badge/NumPy-1.24-orange?logo=numpy)
![License](https://img.shields.io/badge/License-MIT-green)

---

## âœ¨ Highlights
- ğŸ§  **Full Transformer** (Encoder + Decoder) as described in the paper  
- ğŸ¯ **Multi-Head Attention** & **Scaled Dot-Product Attention**  
- ğŸ“ **Sinusoidal Positional Encoding**  
- ğŸ”„ **Residual Connections + Layer Normalization**  
- ğŸ“ˆ **Warmup Learning Rate Schedule** (Noam LR)  
- ğŸ§ª **Toy Copy Task** for quick verification  
- âš¡ Modular codebase â€” easy to extend for translation, summarization, etc.  

---

## ğŸ“– Paper Reference
ğŸ“‘ **Attention Is All You Need**  
*Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin*  
Published at NeurIPS 2017.  
[ğŸ‘‰ Read on arXiv](https://arxiv.org/abs/1706.03762)

---

## ğŸ—ï¸ Project Structure
```bash
transformer-from-scratch/
â”œâ”€â”€ data/ # placeholder for datasets
â”œâ”€â”€ model/
â”‚ â”œâ”€â”€ init.py
â”‚ â”œâ”€â”€ layers.py # Scaled Dot-Product, MultiHeadAttention, FFN, EncoderLayer, DecoderLayer
â”‚ â”œâ”€â”€ transformer.py # Encoder, Decoder, Full Transformer model
â”‚ â”œâ”€â”€ utils.py # Masks, Learning Rate Scheduler
â”œâ”€â”€ train.py # Training loop (toy copy task)
â”œâ”€â”€ evaluate.py # Greedy decoding
â”œâ”€â”€ requirements.txt # Dependencies
â””â”€â”€ README.md 
```
---

## âš™ï¸ Installation

```bash
### Clone the repo
git clone https://github.com/YOUR_USERNAME/transformer-from-scratch.git
cd transformer-from-scratch

### Create conda environment
conda create -n transformer python=3.10 -y
conda activate transformer

### Install dependencies
pip install -r requirements.txt
```
---
## ğŸƒ Training (Toy Copy Task)

We start with a simple copy task: the model learns to output the same sequence it receives.
```bash
python train.py
```
---
## ğŸ“Œ Example training log:

Epoch 1: Loss = 4.01
Epoch 2: Loss = 3.89
Epoch 3: Loss = 3.87
...

âœ… Loss decreases â†’ confirms the Transformer is working.

---

## ğŸ” Evaluation (Greedy Decode)
```bash
python evaluate.py
```
Example:
Input  : [1, 23, 45, 12]
Output : [1, 23, 45, 12]

---
## ğŸ“Š Roadmap
Add Label Smoothing (Îµ = 0.1) as in the paper
Train on real translation datasets (e.g., WMT14 Enâ†”De)
Implement Beam Search decoding (beam size = 4)
Scale up to larger models (d_model=512, 6 layers)
Run on GPU/TPU (Kaggle, Colab, or cloud)

---
## ğŸ› ï¸ Tech Stack
1. PyTorch
2. NumPy
3. tqdm
4. einops

---
